# API Pipeline -- E-commerce Data

Pipeline de Data Engineering que consume datos desde una API REST de
e-commerce, aplica transformaciones y validaciones, y persiste los
resultados en formato Parquet particionado.

El proyecto simula un flujo ETL real:

Extract â†’ Transform â†’ Load

------------------------------------------------------------------------

## ğŸš€ Funcionalidad

El pipeline realiza:

âœ” Consumo de API REST\
âœ” Manejo de errores y reintentos (retry + backoff)\
âœ” Transformaciones con Pandas\
âœ” Validaciones de calidad de datos\
âœ” Escritura en Parquet particionado\
âœ” Logging estructurado

------------------------------------------------------------------------

## âš™ï¸ Setup

1.  Clonar el repositorio

``` bash
git clone https://github.com/tu_usuario/tu_repo.git
cd tu_repo
```

2.  Crear archivo `.env`

``` env
API_TOKEN=tu_token_aqui
API_BASE_URL=https://iansaura.com/api
```

API_BASE_URL es opcional (tiene valor por defecto).

3.  Instalar dependencias

``` bash
pip install -r requirements.txt
```

------------------------------------------------------------------------

## â–¶ï¸ Uso

Ejecutar el pipeline:

``` bash
python main.py
```

------------------------------------------------------------------------

## ğŸ§© Transformaciones aplicadas

El pipeline enriquece los datos de Ã³rdenes agregando:

-   ConversiÃ³n de tipos (datetime, numeric)
-   AÃ±o de la orden (order_year)
-   Mes de la orden (order_month)
-   DÃ­a de la semana (day_of_week)
-   Flag de Ã³rdenes de alto valor (is_high_value)

TambiÃ©n se validan valores invÃ¡lidos en campos numÃ©ricos.

------------------------------------------------------------------------

## ğŸ’¾ Output

Los datos se almacenan en el directorio output/:

output/ â”œâ”€â”€ orders/ â”‚ â”œâ”€â”€ order_year=2024/ â”‚ â”‚ â”œâ”€â”€ order_month=2024-01/
â”‚ â”‚ â”œâ”€â”€ order_month=2024-02/ â”‚ â”‚ â””â”€â”€ ... â””â”€â”€ orders_all.parquet

âœ” Dataset particionado (Ã³ptimo para engines analÃ­ticos)\
âœ” Archivo consolidado

------------------------------------------------------------------------

## ğŸ›  TecnologÃ­as utilizadas

-   Python\
-   Requests\
-   Pandas\
-   Parquet\
-   Logging\
-   dotenv

------------------------------------------------------------------------

## ğŸ“Œ CaracterÃ­sticas de Data Engineering

Este proyecto demuestra prÃ¡cticas comunes en pipelines productivos:

âœ” Idempotencia\
âœ” Retry logic\
âœ” Data validation\
âœ” Schema handling\
âœ” Partitioned storage\
âœ” Observabilidad (logs)

------------------------------------------------------------------------

## ğŸ‘¤ Autor

Octavio Alvarez
